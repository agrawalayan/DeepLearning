{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep LEarning - Fall 2018, \n",
    "#Joshua Matt Abraham - jma672\n",
    "#Ayan Agrawal - aka398"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing pyTorch\n",
    "from os import path\n",
    "from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
    "platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
    "\n",
    "accelerator = 'cu80' if path.exists('/opt/bin/nvidia-smi') else 'cpu'\n",
    "\n",
    "!pip3 install -q http://download.pytorch.org/whl/{accelerator}/torch-0.4.0-{platform}-linux_x86_64.whl torchvision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2.0_3\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "#Importing the data and splitting into train and test datasets\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "print(torch.__version__)\n",
    "transform = transforms.Compose(\n",
    "[ transforms.ToTensor() ,\n",
    "transforms.Normalize(( 0.5 , 0.5 , 0.5), (0.5, 0.5, 0.5 ))])\n",
    "trainset = torchvision.datasets.CIFAR10( root='./data', train=True,\n",
    "download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10( root='./data', train=False,\n",
    "download=True , transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing data to form vectorized training, test and label sets\n",
    "import numpy as np\n",
    "\n",
    "trainLabelArray = []\n",
    "inputTrainMatrix = np.zeros((3072,len(trainset)))\n",
    "\n",
    "testLabelArray = []\n",
    "testMatrix = np.zeros((3072,len(testset)))\n",
    "\n",
    "for i in range(len(trainset)):\n",
    "  trainLabelArray.append(trainset[i][1])\n",
    "  inputTrainMatrix[:,i] = trainset[i][0].numpy().flatten()\n",
    "  \n",
    "for i in range(len(testset)):\n",
    "  testLabelArray.append(testset[i][1])\n",
    "  testMatrix[:,i] = testset[i][0].numpy().flatten()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Iteration Number:  0\n",
      "Cost: 2.3378625807\n",
      "Epoch Iteration Number:  1\n",
      "Epoch Iteration Number:  2\n",
      "Epoch Iteration Number:  3\n",
      "Epoch Iteration Number:  4\n",
      "Epoch Iteration Number:  5\n",
      "Epoch Iteration Number:  6\n",
      "Epoch Iteration Number:  7\n",
      "Epoch Iteration Number:  8\n",
      "Epoch Iteration Number:  9\n",
      "Epoch Iteration Number:  10\n",
      "Cost: 2.12001962126\n",
      "Epoch Iteration Number:  11\n",
      "Epoch Iteration Number:  12\n",
      "Epoch Iteration Number:  13\n",
      "Epoch Iteration Number:  14\n",
      "Epoch Iteration Number:  15\n",
      "Epoch Iteration Number:  16\n",
      "Epoch Iteration Number:  17\n",
      "Epoch Iteration Number:  18\n",
      "Epoch Iteration Number:  19\n",
      "Epoch Iteration Number:  20\n",
      "Cost: 1.98751023895\n",
      "Epoch Iteration Number:  21\n",
      "Epoch Iteration Number:  22\n",
      "Epoch Iteration Number:  23\n",
      "Epoch Iteration Number:  24\n",
      "Epoch Iteration Number:  25\n",
      "Epoch Iteration Number:  26\n",
      "Epoch Iteration Number:  27\n",
      "Epoch Iteration Number:  28\n",
      "Epoch Iteration Number:  29\n",
      "Epoch Iteration Number:  30\n",
      "Cost: 1.88283419655\n",
      "Epoch Iteration Number:  31\n",
      "Epoch Iteration Number:  32\n",
      "Epoch Iteration Number:  33\n",
      "Epoch Iteration Number:  34\n",
      "Epoch Iteration Number:  35\n",
      "Epoch Iteration Number:  36\n",
      "Epoch Iteration Number:  37\n",
      "Epoch Iteration Number:  38\n",
      "Epoch Iteration Number:  39\n",
      "Epoch Iteration Number:  40\n",
      "Cost: 1.76018456578\n",
      "Epoch Iteration Number:  41\n",
      "Epoch Iteration Number:  42\n",
      "Epoch Iteration Number:  43\n",
      "Epoch Iteration Number:  44\n",
      "Epoch Iteration Number:  45\n",
      "Epoch Iteration Number:  46\n",
      "Epoch Iteration Number:  47\n",
      "Epoch Iteration Number:  48\n",
      "Epoch Iteration Number:  49\n",
      "Epoch Iteration Number:  50\n",
      "Cost: 1.64205486585\n",
      "Epoch Iteration Number:  51\n",
      "Epoch Iteration Number:  52\n",
      "Epoch Iteration Number:  53\n",
      "Epoch Iteration Number:  54\n",
      "Epoch Iteration Number:  55\n",
      "Epoch Iteration Number:  56\n",
      "Epoch Iteration Number:  57\n",
      "Epoch Iteration Number:  58\n",
      "Epoch Iteration Number:  59\n",
      "Epoch Iteration Number:  60\n",
      "Cost: 1.52906878628\n",
      "Epoch Iteration Number:  61\n",
      "Epoch Iteration Number:  62\n",
      "Epoch Iteration Number:  63\n",
      "Epoch Iteration Number:  64\n",
      "Epoch Iteration Number:  65\n",
      "Epoch Iteration Number:  66\n",
      "Epoch Iteration Number:  67\n",
      "Epoch Iteration Number:  68\n",
      "Epoch Iteration Number:  69\n",
      "Epoch Iteration Number:  70\n",
      "Cost: 1.47604606717\n",
      "Epoch Iteration Number:  71\n",
      "Epoch Iteration Number:  72\n",
      "Epoch Iteration Number:  73\n",
      "Epoch Iteration Number:  74\n",
      "Epoch Iteration Number:  75\n",
      "Epoch Iteration Number:  76\n",
      "Epoch Iteration Number:  77\n",
      "Epoch Iteration Number:  78\n",
      "Epoch Iteration Number:  79\n",
      "Epoch Iteration Number:  80\n",
      "Cost: 1.47388013897\n",
      "Epoch Iteration Number:  81\n",
      "Epoch Iteration Number:  82\n",
      "Epoch Iteration Number:  83\n",
      "Epoch Iteration Number:  84\n",
      "Epoch Iteration Number:  85\n",
      "Epoch Iteration Number:  86\n",
      "Epoch Iteration Number:  87\n",
      "Epoch Iteration Number:  88\n",
      "Epoch Iteration Number:  89\n",
      "Epoch Iteration Number:  90\n",
      "Cost: 1.40999380423\n",
      "Epoch Iteration Number:  91\n"
     ]
    }
   ],
   "source": [
    "#Fully Connected Neural Network Code and Training\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class NeuralNetwork(object):\n",
    "    def __init__(self, layer_dimensions, drop_prob=0.0, reg_lambda=0.0):\n",
    "        #every time the weights are initialized to same numbers - functionality of seed()\n",
    "        #np.random.seed(1)\n",
    "        self.parameters = {}\n",
    "        self.num_layers = len(layer_dimensions)-1\n",
    "        self.layer_dimensions = layer_dimensions\n",
    "        self.dropoutProb = drop_prob\n",
    "        self.regLambda=reg_lambda\n",
    "        \n",
    "        self.allWeightArray = []\n",
    "        self.biases = []\n",
    "        for i in range(self.num_layers):\n",
    "          self.allWeightArray.append(np.random.randn(layer_dimensions[i], layer_dimensions[i+1])*0.1)\n",
    "          self.biases.append(np.random.randn(layer_dimensions[i+1],1))\n",
    "\n",
    "        \n",
    "    def affineForward(self, A, W, b):\n",
    "        Z = np.dot(W.T,A)+b\n",
    "        cache = (A,W,Z)\n",
    "        return Z,cache\n",
    "        \n",
    "\n",
    "    def activationForward(self, A, activation):\n",
    "      if activation == \"relu\":\n",
    "        return self.relu(A)\n",
    "      elif activation == \"softmax\":\n",
    "        return self.softmax(A)\n",
    "\n",
    "\n",
    "    def relu(self, X):\n",
    "      return np.maximum(0,X)\n",
    "    \n",
    "    def softmax(self, X):\n",
    "      expX = np.exp(X - np.max(X))\n",
    "      return expX/expX.sum(axis=0) \n",
    "            \n",
    "    def dropout(self, A, prob):\n",
    "      dropoutMask = np.random.rand(A.shape[0], A.shape[1])\n",
    "      dropoutMask = dropoutMask<prob\n",
    "      dropoutMask = dropoutMask/(1-prob)\n",
    "      A = np.multiply(A, dropoutMask)    \n",
    "      return A, dropoutMask\n",
    "      \n",
    "               \n",
    "    def forwardPropagation(self, X):\n",
    "      A = X.copy()\n",
    "      cacheSet={}\n",
    "      dropoutMaskSet={}\n",
    "      for i in range(self.num_layers-1):\n",
    "        prevA = A.copy()\n",
    "        Z, cache = self.affineForward(prevA, self.allWeightArray[i], self.biases[i])\n",
    "        cacheSet[i]=cache\n",
    "        A = self.activationForward(Z, 'relu')\n",
    "        \n",
    "        if self.dropoutProb>0:\n",
    "          A, dropoutMask = self.dropout(A, self.dropoutProb)\n",
    "          dropoutMaskSet[i] = dropoutMask\n",
    "        \n",
    "      Z, cache = self.affineForward(A, self.allWeightArray[-1], self.biases[-1])\n",
    "      cacheSet[self.num_layers-1]=cache\n",
    "      A = self.activationForward(Z, 'softmax')   \n",
    "      return A, cacheSet, dropoutMaskSet\n",
    "              \n",
    "    \n",
    "    def costFunction(self, AL, y):\n",
    "      trueProbability = AL[y,range(len(y))]\n",
    "      cost = - np.sum(np.log(trueProbability))/len(y)\n",
    "      cost = np.squeeze(cost)\n",
    "      return cost, self.softmax_derivative(AL, y)\n",
    "                            \n",
    "    \n",
    "    def softmax_derivative(self,AL,Y):\n",
    "      trueOneHot = np.zeros((10,len(Y)))\n",
    "      trueOneHot[Y, range(len(Y))] = 1\n",
    "      return AL-trueOneHot\n",
    "        \n",
    "    def affineBackwardLastLayer(self, dZ, cache):\n",
    "      A,W,Z = cache\n",
    "      m = A.shape[1]\n",
    "      dW = 1/m*(np.dot(A,dZ.T))\n",
    "      db = 1/m*np.sum(dZ, axis=1, keepdims=True)\n",
    "      return (dZ,dW,db)\n",
    "\n",
    "    def affineBackward(self, dZ, cache):\n",
    "      A,W,Z = cache\n",
    "      m = A.shape[1]\n",
    "      dW = 1/m*(np.dot(A,dZ.T))\n",
    "      db = 1/m*np.sum(dZ, axis=1, keepdims=True)\n",
    "      return (dZ, dW, db)\n",
    "        \n",
    "        \n",
    "    def activationBackward(self, cache, activation=\"relu\"):\n",
    "      A,W,Z = cache\n",
    "      return self.relu_derivative(Z)\n",
    "        \n",
    "                \n",
    "    def relu_derivative(self, Z):\n",
    "      Z[Z<=0] = 0\n",
    "      Z[Z>0] = 1\n",
    "      return Z\n",
    "         \n",
    "    \n",
    "    def dropout_backward(self, dA, mask):\n",
    "      dA=np.multiply(dA, mask)\n",
    "      return dA\n",
    "        \n",
    "\n",
    "    def backPropagation(self, dAL, Y, cache, dropoutMaskSet):\n",
    "      deltas = {}\n",
    "      gradTuple = self.affineBackwardLastLayer(dAL, cache[self.num_layers-1])\n",
    "      #Regularization  \n",
    "      if self.regLambda > 0:\n",
    "        dW=gradTuple[1]+np.multiply(self.regLambda,cache[self.num_layers-1][1])/cache[self.num_layers-1][0].shape[1]\n",
    "        gradTuple = (gradTuple[0], dW, gradTuple[2])\n",
    "        \n",
    "      deltas[self.num_layers-1] = gradTuple\n",
    "      \n",
    "      for i in range(self.num_layers-1):\n",
    "        currentCache = cache[len(cache)-i-2]\n",
    "        relu_derivative = self.activationBackward(currentCache)\n",
    "        \n",
    "        dZ = np.multiply(np.dot(cache[len(cache)-i-1][1],deltas[self.num_layers-1-i][0]),relu_derivative)\n",
    "        \n",
    "        gradTuple = self.affineBackward(dZ, currentCache)\n",
    "        \n",
    "        #Dropout Implementation\n",
    "        if self.dropoutProb>0:\n",
    "          dZ=self.dropout_backward(gradTuple[0], dropoutMaskSet[len(dropoutMaskSet)-i-1])\n",
    "          gradTuple = (dZ, gradTuple[1], gradTuple[2])\n",
    "            \n",
    "        #Regularization Implementation\n",
    "        if self.regLambda > 0:\n",
    "          dW=gradTuple[1]+np.multiply(self.regLambda,currentCache[1])/currentCache[0].shape[1]\n",
    "          gradTuple = (gradTuple[0], dW, gradTuple[2])\n",
    "        \n",
    "        deltas[self.num_layers-1-i-1] = gradTuple\n",
    "        \n",
    "      return deltas      \n",
    "      \n",
    "    def updateParameters(self, gradients, alpha):\n",
    "      for i in range(self.num_layers):\n",
    "        self.allWeightArray[i] = self.allWeightArray[i]-alpha*gradients[i][1]\n",
    "        self.biases[i] = self.biases[i]-alpha*gradients[i][2]\n",
    "        \n",
    "     \n",
    "    def train(self, X, y, iters=300, alpha=0.01, batch_size=100, print_every=100):\n",
    "      batchNumbers = int(X.shape[1]/batch_size)\n",
    "      inputTrainBatches = np.hsplit(X, batchNumbers)\n",
    "      n=batch_size\n",
    "      labelBatches = [y[i * n:(i + 1) * n] for i in range((len(y) + n - 1) // n )]\n",
    "      \n",
    "      for j in range(iters):\n",
    "        for i in range(len(inputTrainBatches)):\n",
    "          softmaxY, cacheSet, dropoutMaskSet = self.forwardPropagation(inputTrainBatches[i])\n",
    "\n",
    "          cost, dSoftmaxY = self.costFunction(softmaxY, labelBatches[i])\n",
    "\n",
    "          deltas = self.backPropagation(dSoftmaxY, labelBatches[i], cacheSet, dropoutMaskSet)\n",
    "\n",
    "          self.updateParameters(deltas, alpha)\n",
    "        print(\"Epoch Iteration Number: \",j)\n",
    "        if j%print_every==0:\n",
    "          print(\"Cost:\", cost)\n",
    "          \n",
    "              \n",
    "    def predict(self, X):\n",
    "      softmaxY, cache, dropMask = self.forwardPropagation(X)\n",
    "      predictedLabels = np.argmax(softmaxY, axis=0)\n",
    "      print(predictedLabels)\n",
    "      return predictedLabels\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "#number of layers including input and output, each vlaue represents number of nodes for that layer\n",
    "nnDimensions = [len(inputTrainMatrix), 128, 64, 10]\n",
    "batchSize = 10\n",
    "alpha=0.001\n",
    "printNumber=10\n",
    "iterations=92\n",
    "dropProb=0\n",
    "regLambda=0.01\n",
    "nn = NeuralNetwork(nnDimensions, drop_prob=dropProb, reg_lambda=regLambda)\n",
    "nn.train(inputTrainMatrix, trainLabelArray, iters=iterations, alpha=alpha, batch_size=batchSize, print_every=printNumber)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 1 0 ..., 5 2 7]\n",
      "Test set Accuracy:  51.85999999999999\n"
     ]
    }
   ],
   "source": [
    "predictedLabels = nn.predict(testMatrix)\n",
    "diff = predictedLabels-testLabelArray\n",
    "positiveInstances=np.count_nonzero(diff == 0)\n",
    "accuracy = float(positiveInstances/len(diff))\n",
    "print(\"Test set Accuracy: \",accuracy*100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_predictions(filename, y):\n",
    "    np.save(filename, y)\n",
    "\n",
    "save_predictions(\"ans1-aka398.npy\", predictedLabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
